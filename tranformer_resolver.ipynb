{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tranformer-resolver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimbo/deep-resolver/blob/master/tranformer_resolver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oStlOnQrctGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJtqKxygXUuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "238ae0e3-2d09-432b-cc13-b6bf39a17c73"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMrN9TJkcmOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tPjcbr94CPVY"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pt7NUbJMVK-H"
      },
      "source": [
        "## Model Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ehG_01DVK-J",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Simple linear layers with dropout and relu\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "    \n",
        "class Embeddings(nn.Module):\n",
        "    \"Create word embeddings\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "    \n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module \"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "    \n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bn237Niw3v5R"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The encoder is composed of a stack of $N=6$ identical layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w0jPCmNjpzqQ",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "    \n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward \"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Iwl4OmC8VK-n"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tVNegfOCVK-o",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "    \n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJlAhkFRVK-5"
      },
      "source": [
        "##Implement Attention\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/1706.03762.pdf         \n",
        "                                                                                                                                                                     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D_5M7GA9VK-_",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask):\n",
        "    # Compute 'Scaled Dot Product Attention'\n",
        "    scale = math.sqrt(query.size(-1))\n",
        "\n",
        "    # scores = QK^T/scale\n",
        "    scores = torch.bmm(query, key.transpose(-2, -1)) / scale\n",
        "    # Apply the mask\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "    # output = softmax(scores)(V)\n",
        "    output = F.softmax(scores, dim=-1)\n",
        "    output = torch.bmm(output, value)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mv1rWttbVK_K",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # Implement Multi-head attention mechanism\n",
        "        self.h = h\n",
        "        self.d_k = self.d_v = d_model // h\n",
        "        \n",
        "        # Make an attention head (linear layers for q, k, and v)\n",
        "        # Make h copies of the attention head (Hint: See the `clones()` helper function)\n",
        "        self.w_qs = clones(nn.Linear(d_model, self.d_k), h)\n",
        "        self.w_ks = clones(nn.Linear(d_model, self.d_k), h)\n",
        "        self.w_vs = clones(nn.Linear(d_model, self.d_v), h)\n",
        "\n",
        "        self.proj = nn.Linear(self.d_v * h, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "      residual = query\n",
        "      outputs = []\n",
        "      for w_q, w_k, w_v in zip(self.w_qs, self.w_ks, self.w_vs):\n",
        "        q_i = w_q(query)\n",
        "        k_i = w_k(key)\n",
        "        v_i = w_v(value)\n",
        "        output = attention(q_i, k_i, v_i, mask)\n",
        "        outputs += [output]\n",
        "      out = torch.cat(outputs, dim=2)\n",
        "      out = self.proj(out)\n",
        "      return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "32CAx69kVK_V"
      },
      "source": [
        "## Positional Encoding                                                                                                                             \n",
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](https://arxiv.org/pdf/1705.03122.pdf). \n",
        "\n",
        "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "\n",
        "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$                                                                                                                                                                                                                                                        \n",
        "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. \n",
        "\n",
        "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. \n",
        "                                                                                                                                                                                                                                                    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MZdSqDxuVK_V",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = 1 / (10000 ** (torch.arange(0., d_model, 2) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bZ5w5s1vVK_f"
      },
      "source": [
        "## Full Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oA5SHpQpVK_f",
        "colab": {}
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full transformer model\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=256, d_ff=1024, h=8, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        c = copy.deepcopy\n",
        "        \n",
        "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
        "        self.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))\n",
        "        self.generator = Generator(d_model, tgt_vocab)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X9aZ53S-VK_k"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nM7uHa_0VK_m"
      },
      "source": [
        "## Batches and Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b0gFTX4gVK_n",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask\n",
        "    \n",
        "    \n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sQMYhWxKVK_9"
      },
      "source": [
        "## Label Smoothing\n",
        "\n",
        "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H7My_MENVK_-",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-1e4lIz1VLAT"
      },
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wKFl6R5NVLAi"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5aSC2-z9Gx6Y",
        "colab": {}
      },
      "source": [
        "from torchtext import data, datasets\n",
        "\n",
        "class LossFunction:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        return loss.data * norm\n",
        "\n",
        "class DataIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1).cuda(), batch.trg.transpose(0, 1).cuda()\n",
        "    return Batch(src, trg, pad_idx)\n",
        "\n",
        "    \n",
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T97ls8JtZ_6C"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XXGqjrC-VK-A",
        "outputId": "83365712-4bf2-421b-dce2-e7f3e193d06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# Download DNS queries and responses (questions and answers)\n",
        "!wget  -O ./queries \"https://kimbo.s3-us-west-1.amazonaws.com/dl/current/all-A-questions-only.txt\"\n",
        "!wget -O ./responses \"https://kimbo.s3-us-west-1.amazonaws.com/dl/current/all-A-answers-only.txt\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-13 21:33:49--  https://kimbo.s3-us-west-1.amazonaws.com/dl/current/all-A-questions-only.txt\n",
            "Resolving kimbo.s3-us-west-1.amazonaws.com (kimbo.s3-us-west-1.amazonaws.com)... 52.219.120.81\n",
            "Connecting to kimbo.s3-us-west-1.amazonaws.com (kimbo.s3-us-west-1.amazonaws.com)|52.219.120.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7017655 (6.7M) [text/plain]\n",
            "Saving to: ‘./queries’\n",
            "\n",
            "./queries           100%[===================>]   6.69M  5.22MB/s    in 1.3s    \n",
            "\n",
            "2020-04-13 21:33:51 (5.22 MB/s) - ‘./queries’ saved [7017655/7017655]\n",
            "\n",
            "--2020-04-13 21:33:53--  https://kimbo.s3-us-west-1.amazonaws.com/dl/current/all-A-answers-only.txt\n",
            "Resolving kimbo.s3-us-west-1.amazonaws.com (kimbo.s3-us-west-1.amazonaws.com)... 52.219.112.137\n",
            "Connecting to kimbo.s3-us-west-1.amazonaws.com (kimbo.s3-us-west-1.amazonaws.com)|52.219.112.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18247656 (17M) [text/plain]\n",
            "Saving to: ‘./responses’\n",
            "\n",
            "./responses         100%[===================>]  17.40M  9.58MB/s    in 1.8s    \n",
            "\n",
            "2020-04-13 21:33:55 (9.58 MB/s) - ‘./responses’ saved [18247656/18247656]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6GsSdf1B2ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "997b237b-03be-4149-c9f5-160836ce1300"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vk6oMyrxvMXk",
        "outputId": "c7df0fb9-64f7-4046-e6ac-150cd31ce55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from torchtext import data, datasets\n",
        "import torchtext\n",
        "import spacy\n",
        "import pdb\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return [tok.text for tok in nlp.tokenizer(text)]\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC = data.Field(tokenize=tokenize, pad_token=BLANK_WORD)\n",
        "TGT = data.Field(tokenize=tokenize, init_token=BOS_WORD, \n",
        "                 eos_token=EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "print(\"Loading Dataset...\")\n",
        "limit = 100000\n",
        "with open('./queries', 'r') as qf, open('./responses', 'r') as rf:\n",
        "  query_lines = list(map(lambda l: l.strip().strip('?'), qf))[:limit]\n",
        "  response_lines = list(map(lambda l: l.strip(), rf))[:limit]\n",
        "\n",
        "assert len(query_lines) == len(response_lines)\n",
        "\n",
        "print('Loaded {} query/response pairs'.format(len(response_lines)))\n",
        "print('Example query: {}'.format(query_lines[0]))\n",
        "print('Example response: {}'.format(response_lines[0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Dataset...\n",
            "Loaded 100000 query/response pairs\n",
            "Example query: ipayment.com. A\n",
            "Example response: ipayment.com. 600 IN A 184.168.131.241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7skxTzca-eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5bd59434-a3e8-4719-d830-df3a123ff317"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "fields = ([\"src\", SRC], [\"trg\", TGT])\n",
        "examples = [torchtext.data.Example.fromlist((query_lines[i], response_lines[i]), fields) \n",
        "            for i in tqdm.trange(len(response_lines), position=0, desc='Creating examples...')]\n",
        "\n",
        "print('Creating torchtext.data.Dataset...')\n",
        "train, val = torchtext.data.Dataset(examples, fields=fields).split()\n",
        "print('Building vocab...')\n",
        "MIN_FREQ = 1\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "print('SRC vocab done.')\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
        "print('TGT vocab done.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating examples...: 100%|██████████| 100000/100000 [00:32<00:00, 3089.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating torchtext.data.Dataset...\n",
            "Building vocab...\n",
            "SRC vocab done.\n",
            "TGT vocab done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ejIuRQylVLAm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bd14ee5-1516-48e2-b7d4-2fbbb4c2d6b9"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "N = 2\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = TransformerModel(len(SRC.vocab), len(TGT.vocab), N=N).cuda()\n",
        "n_epochs = 100\n",
        "device = torch.device('cuda')\n",
        "lr = 5e-4\n",
        "# lr = 0.003\n",
        "\n",
        "def scope():\n",
        "    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "    criterion.cuda()\n",
        "    BATCH_SIZE = 1000\n",
        "    train_iter = DataIterator(train, batch_size=BATCH_SIZE, device=device,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "    valid_iter = DataIterator(val, batch_size=BATCH_SIZE, device=device,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "    model_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model, \n",
        "                  LossFunction(model.generator, criterion, model_opt))\n",
        "        model.eval()\n",
        "scope()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 1 Loss: 10.243751 Tokens per Sec: 2480.402588\n",
            "Epoch Step: 51 Loss: 4.558431 Tokens per Sec: 4216.205566\n",
            "Epoch Step: 101 Loss: 3.945638 Tokens per Sec: 4255.244141\n",
            "Epoch Step: 151 Loss: 3.607424 Tokens per Sec: 4184.705078\n",
            "Epoch Step: 201 Loss: 3.465854 Tokens per Sec: 4261.685547\n",
            "Epoch Step: 251 Loss: 3.448779 Tokens per Sec: 4140.226074\n",
            "Epoch Step: 301 Loss: 3.491423 Tokens per Sec: 4172.465332\n",
            "Epoch Step: 351 Loss: 3.414208 Tokens per Sec: 4173.915039\n",
            "Epoch Step: 401 Loss: 2.993244 Tokens per Sec: 4152.592285\n",
            "Epoch Step: 451 Loss: 3.395380 Tokens per Sec: 4241.622559\n",
            "Epoch Step: 501 Loss: 3.394483 Tokens per Sec: 4076.358887\n",
            "Epoch Step: 551 Loss: 3.392555 Tokens per Sec: 4181.548828\n",
            "Epoch Step: 601 Loss: 2.877407 Tokens per Sec: 4294.947754\n",
            "Epoch Step: 1 Loss: 3.416408 Tokens per Sec: 2533.921631\n",
            "Epoch Step: 51 Loss: 3.446699 Tokens per Sec: 4246.486328\n",
            "Epoch Step: 101 Loss: 3.439760 Tokens per Sec: 4260.419922\n",
            "Epoch Step: 151 Loss: 3.466607 Tokens per Sec: 4132.993652\n",
            "Epoch Step: 201 Loss: 3.419544 Tokens per Sec: 4018.691895\n",
            "Epoch Step: 251 Loss: 3.418179 Tokens per Sec: 4047.516602\n",
            "Epoch Step: 301 Loss: 3.419499 Tokens per Sec: 4233.668457\n",
            "Epoch Step: 351 Loss: 3.439391 Tokens per Sec: 4093.116211\n",
            "Epoch Step: 401 Loss: 3.436483 Tokens per Sec: 4255.193848\n",
            "Epoch Step: 451 Loss: 2.998676 Tokens per Sec: 4238.305664\n",
            "Epoch Step: 501 Loss: 3.058290 Tokens per Sec: 4302.205566\n",
            "Epoch Step: 551 Loss: 3.424007 Tokens per Sec: 4138.228516\n",
            "Epoch Step: 601 Loss: 3.408330 Tokens per Sec: 4322.154297\n",
            "Epoch Step: 1 Loss: 3.410842 Tokens per Sec: 2566.783203\n",
            "Epoch Step: 51 Loss: 3.282524 Tokens per Sec: 4206.062988\n",
            "Epoch Step: 101 Loss: 3.296108 Tokens per Sec: 4255.115723\n",
            "Epoch Step: 151 Loss: 3.339949 Tokens per Sec: 4330.231445\n",
            "Epoch Step: 201 Loss: 3.301180 Tokens per Sec: 4328.277344\n",
            "Epoch Step: 251 Loss: 3.337063 Tokens per Sec: 4230.034668\n",
            "Epoch Step: 301 Loss: 2.920196 Tokens per Sec: 4321.482422\n",
            "Epoch Step: 351 Loss: 2.738962 Tokens per Sec: 4277.809570\n",
            "Epoch Step: 401 Loss: 3.331454 Tokens per Sec: 4308.496094\n",
            "Epoch Step: 451 Loss: 3.311378 Tokens per Sec: 4210.044434\n",
            "Epoch Step: 501 Loss: 3.334973 Tokens per Sec: 4339.782715\n",
            "Epoch Step: 551 Loss: 2.769789 Tokens per Sec: 4194.453613\n",
            "Epoch Step: 601 Loss: 3.300225 Tokens per Sec: 4273.140625\n",
            "Epoch Step: 1 Loss: 3.318384 Tokens per Sec: 2413.221191\n",
            "Epoch Step: 51 Loss: 2.611135 Tokens per Sec: 4179.793945\n",
            "Epoch Step: 101 Loss: 3.079203 Tokens per Sec: 4173.566406\n",
            "Epoch Step: 151 Loss: 3.153473 Tokens per Sec: 4110.117188\n",
            "Epoch Step: 201 Loss: 2.560513 Tokens per Sec: 4199.331543\n",
            "Epoch Step: 251 Loss: 3.113234 Tokens per Sec: 4238.083008\n",
            "Epoch Step: 301 Loss: 2.528459 Tokens per Sec: 4275.173828\n",
            "Epoch Step: 351 Loss: 3.057028 Tokens per Sec: 4263.415039\n",
            "Epoch Step: 401 Loss: 3.068979 Tokens per Sec: 4238.950684\n",
            "Epoch Step: 451 Loss: 3.126071 Tokens per Sec: 4255.890625\n",
            "Epoch Step: 501 Loss: 3.071012 Tokens per Sec: 4271.982910\n",
            "Epoch Step: 551 Loss: 2.654565 Tokens per Sec: 4200.348145\n",
            "Epoch Step: 601 Loss: 3.049406 Tokens per Sec: 4155.716797\n",
            "Epoch Step: 1 Loss: 2.271328 Tokens per Sec: 2434.041504\n",
            "Epoch Step: 51 Loss: 2.334767 Tokens per Sec: 4232.549316\n",
            "Epoch Step: 101 Loss: 2.289834 Tokens per Sec: 4197.120605\n",
            "Epoch Step: 151 Loss: 2.216486 Tokens per Sec: 4248.329102\n",
            "Epoch Step: 201 Loss: 2.267867 Tokens per Sec: 4274.244629\n",
            "Epoch Step: 251 Loss: 2.728501 Tokens per Sec: 4311.986328\n",
            "Epoch Step: 301 Loss: 2.637216 Tokens per Sec: 4317.486816\n",
            "Epoch Step: 351 Loss: 2.747900 Tokens per Sec: 4179.307617\n",
            "Epoch Step: 401 Loss: 2.641852 Tokens per Sec: 4303.437012\n",
            "Epoch Step: 451 Loss: 2.730879 Tokens per Sec: 4156.974609\n",
            "Epoch Step: 501 Loss: 2.351990 Tokens per Sec: 4335.303711\n",
            "Epoch Step: 551 Loss: 2.362719 Tokens per Sec: 4220.898926\n",
            "Epoch Step: 601 Loss: 2.857439 Tokens per Sec: 4204.970703\n",
            "Epoch Step: 1 Loss: 1.866442 Tokens per Sec: 2671.726318\n",
            "Epoch Step: 51 Loss: 2.215255 Tokens per Sec: 4336.018066\n",
            "Epoch Step: 101 Loss: 1.845041 Tokens per Sec: 4329.794922\n",
            "Epoch Step: 151 Loss: 2.256273 Tokens per Sec: 4351.431641\n",
            "Epoch Step: 201 Loss: 1.885289 Tokens per Sec: 4366.776855\n",
            "Epoch Step: 251 Loss: 2.315903 Tokens per Sec: 4310.254395\n",
            "Epoch Step: 301 Loss: 1.961841 Tokens per Sec: 4209.827148\n",
            "Epoch Step: 351 Loss: 1.949787 Tokens per Sec: 4356.632812\n",
            "Epoch Step: 401 Loss: 2.360791 Tokens per Sec: 4200.089355\n",
            "Epoch Step: 451 Loss: 1.901344 Tokens per Sec: 4250.639160\n",
            "Epoch Step: 501 Loss: 2.347756 Tokens per Sec: 4344.447754\n",
            "Epoch Step: 551 Loss: 2.340154 Tokens per Sec: 4222.154785\n",
            "Epoch Step: 601 Loss: 2.425331 Tokens per Sec: 4235.181152\n",
            "Epoch Step: 1 Loss: 1.749429 Tokens per Sec: 2556.784424\n",
            "Epoch Step: 51 Loss: 1.658425 Tokens per Sec: 4321.148438\n",
            "Epoch Step: 101 Loss: 1.727439 Tokens per Sec: 4261.436035\n",
            "Epoch Step: 151 Loss: 1.756484 Tokens per Sec: 4331.172852\n",
            "Epoch Step: 201 Loss: 1.757059 Tokens per Sec: 4244.200195\n",
            "Epoch Step: 251 Loss: 1.821925 Tokens per Sec: 4315.361816\n",
            "Epoch Step: 301 Loss: 1.839610 Tokens per Sec: 4318.304688\n",
            "Epoch Step: 351 Loss: 1.878174 Tokens per Sec: 4261.786621\n",
            "Epoch Step: 401 Loss: 1.859608 Tokens per Sec: 4323.888672\n",
            "Epoch Step: 451 Loss: 1.585264 Tokens per Sec: 4167.427246\n",
            "Epoch Step: 501 Loss: 1.896992 Tokens per Sec: 4341.444336\n",
            "Epoch Step: 551 Loss: 1.887754 Tokens per Sec: 4247.675293\n",
            "Epoch Step: 601 Loss: 1.939538 Tokens per Sec: 4250.253906\n",
            "Epoch Step: 1 Loss: 1.175423 Tokens per Sec: 2631.029297\n",
            "Epoch Step: 51 Loss: 1.246829 Tokens per Sec: 4229.469238\n",
            "Epoch Step: 101 Loss: 1.246044 Tokens per Sec: 4333.910645\n",
            "Epoch Step: 151 Loss: 1.054070 Tokens per Sec: 4303.939941\n",
            "Epoch Step: 201 Loss: 1.132682 Tokens per Sec: 4341.956055\n",
            "Epoch Step: 251 Loss: 1.341514 Tokens per Sec: 4215.380859\n",
            "Epoch Step: 301 Loss: 1.139689 Tokens per Sec: 4258.786621\n",
            "Epoch Step: 351 Loss: 1.322458 Tokens per Sec: 4314.910645\n",
            "Epoch Step: 401 Loss: 1.411041 Tokens per Sec: 4346.464844\n",
            "Epoch Step: 451 Loss: 1.435066 Tokens per Sec: 4239.924316\n",
            "Epoch Step: 501 Loss: 1.416420 Tokens per Sec: 4208.864746\n",
            "Epoch Step: 551 Loss: 1.243915 Tokens per Sec: 4132.893066\n",
            "Epoch Step: 601 Loss: 1.455795 Tokens per Sec: 4095.946777\n",
            "Epoch Step: 1 Loss: 0.631098 Tokens per Sec: 2621.059082\n",
            "Epoch Step: 51 Loss: 0.699266 Tokens per Sec: 4138.801270\n",
            "Epoch Step: 101 Loss: 0.686748 Tokens per Sec: 4157.936523\n",
            "Epoch Step: 151 Loss: 0.727838 Tokens per Sec: 4311.461914\n",
            "Epoch Step: 201 Loss: 0.788411 Tokens per Sec: 4288.646973\n",
            "Epoch Step: 251 Loss: 0.805332 Tokens per Sec: 4273.796875\n",
            "Epoch Step: 301 Loss: 0.849987 Tokens per Sec: 4346.375488\n",
            "Epoch Step: 351 Loss: 0.752132 Tokens per Sec: 4201.492188\n",
            "Epoch Step: 401 Loss: 0.769094 Tokens per Sec: 4334.166992\n",
            "Epoch Step: 451 Loss: 0.884879 Tokens per Sec: 4286.967285\n",
            "Epoch Step: 501 Loss: 0.760532 Tokens per Sec: 4150.856934\n",
            "Epoch Step: 551 Loss: 0.887498 Tokens per Sec: 4170.163086\n",
            "Epoch Step: 601 Loss: 0.964019 Tokens per Sec: 4125.735352\n",
            "Epoch Step: 1 Loss: 0.302091 Tokens per Sec: 2438.564209\n",
            "Epoch Step: 51 Loss: 0.294720 Tokens per Sec: 4251.485352\n",
            "Epoch Step: 101 Loss: 0.303604 Tokens per Sec: 4323.044434\n",
            "Epoch Step: 151 Loss: 0.348899 Tokens per Sec: 4237.613770\n",
            "Epoch Step: 201 Loss: 0.319935 Tokens per Sec: 4340.254883\n",
            "Epoch Step: 251 Loss: 0.349654 Tokens per Sec: 4272.582031\n",
            "Epoch Step: 301 Loss: 0.304044 Tokens per Sec: 4322.033203\n",
            "Epoch Step: 351 Loss: 0.351976 Tokens per Sec: 4286.183594\n",
            "Epoch Step: 401 Loss: 0.422808 Tokens per Sec: 4347.577148\n",
            "Epoch Step: 451 Loss: 0.408767 Tokens per Sec: 4273.677734\n",
            "Epoch Step: 501 Loss: 0.344449 Tokens per Sec: 4382.173828\n",
            "Epoch Step: 551 Loss: 0.441800 Tokens per Sec: 4395.581055\n",
            "Epoch Step: 601 Loss: 0.452552 Tokens per Sec: 4352.856445\n",
            "Epoch Step: 1 Loss: 0.153155 Tokens per Sec: 2606.599121\n",
            "Epoch Step: 51 Loss: 0.158160 Tokens per Sec: 4285.819824\n",
            "Epoch Step: 101 Loss: 0.169025 Tokens per Sec: 4323.266602\n",
            "Epoch Step: 151 Loss: 0.172689 Tokens per Sec: 4403.312988\n",
            "Epoch Step: 201 Loss: 0.170489 Tokens per Sec: 4238.483398\n",
            "Epoch Step: 251 Loss: 0.153129 Tokens per Sec: 4366.446289\n",
            "Epoch Step: 301 Loss: 0.169289 Tokens per Sec: 4355.482910\n",
            "Epoch Step: 351 Loss: 0.167074 Tokens per Sec: 4333.622070\n",
            "Epoch Step: 401 Loss: 0.174931 Tokens per Sec: 4275.994141\n",
            "Epoch Step: 451 Loss: 0.162587 Tokens per Sec: 4186.425781\n",
            "Epoch Step: 501 Loss: 0.190124 Tokens per Sec: 4248.009766\n",
            "Epoch Step: 551 Loss: 0.182131 Tokens per Sec: 4292.514160\n",
            "Epoch Step: 601 Loss: 0.194482 Tokens per Sec: 4247.180176\n",
            "Epoch Step: 1 Loss: 0.149641 Tokens per Sec: 2432.899902\n",
            "Epoch Step: 51 Loss: 0.138939 Tokens per Sec: 4305.151367\n",
            "Epoch Step: 101 Loss: 0.128836 Tokens per Sec: 4279.340820\n",
            "Epoch Step: 151 Loss: 0.142466 Tokens per Sec: 4185.954102\n",
            "Epoch Step: 201 Loss: 0.132276 Tokens per Sec: 4201.258789\n",
            "Epoch Step: 251 Loss: 0.153071 Tokens per Sec: 4285.543457\n",
            "Epoch Step: 301 Loss: 0.148948 Tokens per Sec: 4298.679199\n",
            "Epoch Step: 351 Loss: 0.152878 Tokens per Sec: 4331.925293\n",
            "Epoch Step: 401 Loss: 0.164704 Tokens per Sec: 4273.347656\n",
            "Epoch Step: 451 Loss: 0.151324 Tokens per Sec: 4303.333984\n",
            "Epoch Step: 501 Loss: 0.158116 Tokens per Sec: 4349.463867\n",
            "Epoch Step: 551 Loss: 0.150418 Tokens per Sec: 4337.667480\n",
            "Epoch Step: 601 Loss: 0.158618 Tokens per Sec: 4412.065430\n",
            "Epoch Step: 1 Loss: 0.145192 Tokens per Sec: 2465.027100\n",
            "Epoch Step: 51 Loss: 0.135338 Tokens per Sec: 4396.291016\n",
            "Epoch Step: 101 Loss: 0.146132 Tokens per Sec: 4365.055664\n",
            "Epoch Step: 151 Loss: 0.144965 Tokens per Sec: 4233.475586\n",
            "Epoch Step: 201 Loss: 0.142389 Tokens per Sec: 4309.870117\n",
            "Epoch Step: 251 Loss: 0.137525 Tokens per Sec: 4422.228516\n",
            "Epoch Step: 301 Loss: 0.154967 Tokens per Sec: 4374.269043\n",
            "Epoch Step: 351 Loss: 0.146949 Tokens per Sec: 4322.591797\n",
            "Epoch Step: 401 Loss: 0.140386 Tokens per Sec: 4362.593262\n",
            "Epoch Step: 451 Loss: 0.140641 Tokens per Sec: 4320.436523\n",
            "Epoch Step: 501 Loss: 0.156009 Tokens per Sec: 4396.373047\n",
            "Epoch Step: 551 Loss: 0.234453 Tokens per Sec: 4417.637695\n",
            "Epoch Step: 601 Loss: 0.152069 Tokens per Sec: 4397.111816\n",
            "Epoch Step: 1 Loss: 0.144681 Tokens per Sec: 2545.212646\n",
            "Epoch Step: 51 Loss: 0.138422 Tokens per Sec: 4254.603027\n",
            "Epoch Step: 101 Loss: 0.994911 Tokens per Sec: 4342.499512\n",
            "Epoch Step: 151 Loss: 0.786477 Tokens per Sec: 4334.361816\n",
            "Epoch Step: 201 Loss: 0.131604 Tokens per Sec: 4424.750000\n",
            "Epoch Step: 251 Loss: 0.138857 Tokens per Sec: 4325.783691\n",
            "Epoch Step: 301 Loss: 0.141094 Tokens per Sec: 4372.624023\n",
            "Epoch Step: 351 Loss: 0.153586 Tokens per Sec: 4292.331055\n",
            "Epoch Step: 401 Loss: 0.150362 Tokens per Sec: 4362.220215\n",
            "Epoch Step: 451 Loss: 0.130209 Tokens per Sec: 4263.804199\n",
            "Epoch Step: 501 Loss: 0.151552 Tokens per Sec: 4276.318359\n",
            "Epoch Step: 551 Loss: 0.143712 Tokens per Sec: 4305.509766\n",
            "Epoch Step: 601 Loss: 0.155507 Tokens per Sec: 4336.934082\n",
            "Epoch Step: 1 Loss: 0.144569 Tokens per Sec: 2642.256836\n",
            "Epoch Step: 51 Loss: 0.141150 Tokens per Sec: 4300.596680\n",
            "Epoch Step: 101 Loss: 0.168658 Tokens per Sec: 4255.997070\n",
            "Epoch Step: 151 Loss: 0.130111 Tokens per Sec: 4399.817871\n",
            "Epoch Step: 201 Loss: 0.145380 Tokens per Sec: 4387.262207\n",
            "Epoch Step: 251 Loss: 0.177282 Tokens per Sec: 4378.308594\n",
            "Epoch Step: 301 Loss: 0.145097 Tokens per Sec: 4301.690430\n",
            "Epoch Step: 351 Loss: 0.152754 Tokens per Sec: 4399.673340\n",
            "Epoch Step: 401 Loss: 0.152005 Tokens per Sec: 4353.289062\n",
            "Epoch Step: 451 Loss: 0.175799 Tokens per Sec: 4211.267090\n",
            "Epoch Step: 501 Loss: 0.164967 Tokens per Sec: 4237.213379\n",
            "Epoch Step: 551 Loss: 0.170002 Tokens per Sec: 4366.862793\n",
            "Epoch Step: 601 Loss: 0.160707 Tokens per Sec: 4374.845215\n",
            "Epoch Step: 1 Loss: 0.165028 Tokens per Sec: 2368.590332\n",
            "Epoch Step: 51 Loss: 0.137228 Tokens per Sec: 4353.709961\n",
            "Epoch Step: 101 Loss: 0.152591 Tokens per Sec: 4344.700195\n",
            "Epoch Step: 151 Loss: 0.139571 Tokens per Sec: 4441.029297\n",
            "Epoch Step: 201 Loss: 0.146873 Tokens per Sec: 4403.120605\n",
            "Epoch Step: 251 Loss: 0.153090 Tokens per Sec: 4307.772461\n",
            "Epoch Step: 301 Loss: 0.131944 Tokens per Sec: 4444.107422\n",
            "Epoch Step: 351 Loss: 0.131464 Tokens per Sec: 4251.301758\n",
            "Epoch Step: 401 Loss: 0.167996 Tokens per Sec: 4120.389648\n",
            "Epoch Step: 451 Loss: 0.164615 Tokens per Sec: 4157.674805\n",
            "Epoch Step: 501 Loss: 0.169622 Tokens per Sec: 4010.813232\n",
            "Epoch Step: 551 Loss: 0.143944 Tokens per Sec: 4136.870117\n",
            "Epoch Step: 601 Loss: 0.175062 Tokens per Sec: 4013.220215\n",
            "Epoch Step: 1 Loss: 0.172468 Tokens per Sec: 2520.891602\n",
            "Epoch Step: 51 Loss: 0.139987 Tokens per Sec: 4099.792969\n",
            "Epoch Step: 101 Loss: 0.157880 Tokens per Sec: 4068.168457\n",
            "Epoch Step: 151 Loss: 0.166009 Tokens per Sec: 4038.148438\n",
            "Epoch Step: 201 Loss: 0.448258 Tokens per Sec: 4164.077148\n",
            "Epoch Step: 251 Loss: 0.153145 Tokens per Sec: 4052.562744\n",
            "Epoch Step: 301 Loss: 0.150917 Tokens per Sec: 4084.139404\n",
            "Epoch Step: 351 Loss: 0.145101 Tokens per Sec: 4097.605469\n",
            "Epoch Step: 401 Loss: 0.156009 Tokens per Sec: 4115.644531\n",
            "Epoch Step: 451 Loss: 0.153721 Tokens per Sec: 4149.393066\n",
            "Epoch Step: 501 Loss: 0.144521 Tokens per Sec: 4052.937988\n",
            "Epoch Step: 551 Loss: 0.161801 Tokens per Sec: 4123.536133\n",
            "Epoch Step: 601 Loss: 0.163214 Tokens per Sec: 4032.986084\n",
            "Epoch Step: 1 Loss: 0.172431 Tokens per Sec: 2509.917236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ce24b9386a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                   LossFunction(model.generator, criterion, model_opt))\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-ce24b9386a02>\u001b[0m in \u001b[0;36mscope\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         run_epoch((rebatch(pad_idx, b) for b in train_iter), \n\u001b[1;32m     27\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                   LossFunction(model.generator, criterion, model_opt))\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-59ef021f5251>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m     51\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m     52\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-59ef021f5251>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m     12\u001b[0m         loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n\u001b[1;32m     13\u001b[0m                               y.contiguous().view(-1)) / norm\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TgZTmtfBVLAo"
      },
      "source": [
        "## Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zW0krxegxdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad8fb3a3-7193-410c-e08e-eeadc28b3342"
      },
      "source": [
        "!pip install dnspython"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.6/dist-packages (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnvphK7jpArO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "e70b3716-abd4-4447-a5aa-12ab7dd9dcb4"
      },
      "source": [
        "torch.save(model, 'deepresolver-model.pt')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "92c7c565-8d71-4b81-9428-cf8fffdc84ec",
        "id": "e3wG7Qbvi07M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "import pdb\n",
        "import dns.message\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys\n",
        "\n",
        "BATCH_SIZE = 1000\n",
        "n_train_iters = len(train) / BATCH_SIZE\n",
        "valid_iter = DataIterator(val, batch_size=BATCH_SIZE, device=device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    src = batch.src.transpose(0, 1)[:1].cuda()\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2).cuda()\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=305, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Query:\", end=\"\\t\\t\\t\\t\")\n",
        "    q = ''\n",
        "    for i in range(0, src.size(1)):\n",
        "        sym = SRC.vocab.itos[src[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        q += sym\n",
        "        if i > 0:\n",
        "          q += \" \"\n",
        "        #print(sym, end =\" \")\n",
        "    print(q)\n",
        "    print()\n",
        "    print(\"Generated response:\", end=\"\\t\\t\")\n",
        "    s = \"\\n\"\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "        s += sym\n",
        "        if i > 1:\n",
        "          s += \" \"\n",
        "          print(\" \", end=\"\")\n",
        "    # try:\n",
        "    #   msg = dns.message.from_text(s)\n",
        "    # except Exception as e:\n",
        "    #   print('ERROR DECODING DNS MESSAGE: {}'.format(e))\n",
        "    #   print('\\n{}'.format(s))\n",
        "    # else:\n",
        "    #   print(\"SUCCESS!!!\\n{}\".format(msg))\n",
        "    print('\\n')\n",
        "    print(\"Target response:\\t\", end=\"\\t\")\n",
        "    r = \"\"\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        #print(sym, end =\" \")\n",
        "        r += sym\n",
        "        if i > 1:\n",
        "          r += \" \"\n",
        "    print(r)\n",
        "    print()\n",
        "    \n",
        "    if i > 1000 and i < 1100:\n",
        "        break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query:\t\t\t\t<unk>. A \n",
            "\n",
            "Generated response:\t\tsharevideo1.com .  600  IN  A  162.242.193.45  \n",
            "\n",
            "Target response:\t\t<unk>. 21600 IN A <unk> \n",
            "\n",
            "Query:\t\t\t\t<unk>. A \n",
            "\n",
            "Generated response:\t\telcheapo.tv .  14400  IN  A  162.241.219.194  \n",
            "\n",
            "Target response:\t\t<unk>. 600 IN A <unk> \n",
            "\n",
            "Query:\t\t\t\t<unk>. A \n",
            "\n",
            "Generated response:\t\tsharevideo1.com .  600  IN  A  35.231.33.159  \n",
            "\n",
            "Target response:\t\t<unk>. 21599 IN A <unk> \n",
            "\n",
            "Query:\t\t\t\t<unk>. A \n",
            "\n",
            "Generated response:\t\ttritonsws.com .  3600  IN  A  50.62.57.138  \n",
            "\n",
            "Target response:\t\t<unk>. 30 IN A <unk> \n",
            "\n",
            "Query:\t\t\t\t<unk>. A \n",
            "\n",
            "Generated response:\t\telcheapo.tv .  14400  IN  A  162.208.49.126  \n",
            "\n",
            "Target response:\t\t<unk>. 7200 IN A <unk> \n",
            "\n",
            "Query:\t\t\t\t<unk>. A \n",
            "\n",
            "Generated response:\t\tlatinbayarea.com .  7200  IN  A  206.188.192.247  \n",
            "\n",
            "Target response:\t\t<unk>. 1800 IN A <unk> \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-483fd1ba6c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<blank>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     out = greedy_decode(model, src, src_mask, \n\u001b[0;32m---> 29\u001b[0;31m                         max_len=305, start_symbol=TGT.vocab.stoi[\"<s>\"])\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Query:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\\t\\t\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-483fd1ba6c38>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(model, src, src_mask, max_len, start_symbol)\u001b[0m\n\u001b[1;32m      9\u001b[0m                            \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                            Variable(subsequent_mask(ys.size(1))\n\u001b[0;32m---> 11\u001b[0;31m                                     .type_as(src.data)))\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-06582146a7ac>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, memory, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a48d279d1123>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a48d279d1123>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m\"Follow Figure 1 (right) for connections.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c1f8275b2cba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"Apply residual connection to any sublayer with the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a48d279d1123>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m\"Follow Figure 1 (right) for connections.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c0de7b2de484>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mk_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mv_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-46e4837ffd1e>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(query, key, value, mask)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# scores = QK^T/scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Apply the mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n7ugtK1tVLBD"
      },
      "source": [
        "<div id=\"disqus_thread\"></div>\n",
        "<script>\n",
        "    /**\n",
        "     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n",
        "     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables\n",
        "     */\n",
        "    /*\n",
        "    var disqus_config = function () {\n",
        "        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable\n",
        "        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n",
        "    };\n",
        "    */\n",
        "    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW\n",
        "        var d = document, s = d.createElement('script');\n",
        "        \n",
        "        s.src = 'https://EXAMPLE.disqus.com/embed.js';  // IMPORTANT: Replace EXAMPLE with your forum shortname!\n",
        "        \n",
        "        s.setAttribute('data-timestamp', +new Date());\n",
        "        (d.head || d.body).appendChild(s);\n",
        "    })();\n",
        "</script>\n",
        "<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\" rel=\"nofollow\">comments powered by Disqus.</a></noscript>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-J5TG5Z-Xmsr",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}